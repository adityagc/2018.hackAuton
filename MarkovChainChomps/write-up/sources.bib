@article{IBM-machinefailure,
title = {Are your preventive maintenance efforts wrenching away precious time and resources?},
author = {IBM},
url = {https://www.ibm.com/blogs/internet-of-things/wp-content/uploads/2016/05/IG_preventiveMaintenance15_ss.pdf}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Murray2005,
abstract = {We compare machine learning methods applied to a difficult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classifier (mi-NB) which is specifically designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum significance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures.},
author = {Murray, Joseph F and Hughes, Gordon F and Kreutz-Delgado, Kenneth},
doi = {10.1.1.84.9557},
file = {:home/matt/Documents/Mendeley Desktop/Murray, Hughes, Kreutz-Delgado - 2005 - Machine Learning Methods for Predicting Failures in Hard Drives A Multiple-Instance Application.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {exact,hard drive failure prediction,multiple instance naive-bayes,nonparametric statistics,rank-sum test,support vector machines,svm},
mendeley-groups = {2018-hackAuton},
pages = {783--816},
title = {{Machine Learning Methods for Predicting Failures in Hard Drives: A Multiple-Instance Application}},
url = {http://jmlr.csail.mit.edu/papers/volume6/murray05a/murray05a.pdf http://dl.acm.org/citation.cfm?id=1046920.1088699},
volume = {6},
year = {2005}
}

@techreport{Turnbull2003,
abstract = {server failure},
author = {Turnbull, Doug and Alldrin, Neil},
booktitle = {UCSD CSE221 Project},
file = {:home/matt/Documents/Mendeley Desktop/Turnbull, Alldrin - 2003 - Failure prediction in hardware systems.pdf:pdf},
mendeley-groups = {2018-hackAuton},
title = {{Failure prediction in hardware systems}},
url = {https://cseweb.ucsd.edu/{~}dturnbul/Papers/ServerPrediction.pdf http://cseweb.ucsd.edu/{~}dturnbul/Papers/ServerPrediction.pdf},
year = {2003}
}

@article{Cortes1995,
author={Cortes, Corinna and Vapnik, Vladimir},
title="Support-vector networks",
journal="Machine Learning",
year={1995},
month="Sep",
day="01",
volume="20",
number="3",
pages="273--297",
abstract="Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.",
issn="1573-0565",
doi="10.1007/BF00994018",
url="https://doi.org/10.1007/BF00994018"
}

@incollection{Shalizi2012,
title = {{Logistic Regression}},
booktitle = {{Advanced Data Analysis from an Elementary Point of View}},
author = {Cosma Rohilla Shalizi},
year = {2012},
publisher = {Cambridge University Press},
chapter = {{12 Logistic Regression}},
}

@misc{2018HackAuton,
  author = {Battifarano, M., DeSmet, D., Madabhushi, A., Nabar, P.},
  title = {Predicting Future Machine Failure from Machine State Using Logistic Regression},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/mbattifarano/2018.hackAuton}}
}

@article{Kluyver2016,
abstract = {It is increasingly necessary for researchers in all fields to write computer code, and in order to reproduce research results, it is important that this code is published. We present Jupyter notebooks, a document format for publishing code, results and explanations in a form that is both readable and executable. We discuss various tools and use cases for notebook documents.},
author = {Kluyver, Thomas and Ragan-kelley, Benjamin and P{\'{e}}rez, Fernando and Granger, Brian and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica and Grout, Jason and Corlay, Sylvain and Ivanov, Paul and Avila, Dami{\'{a}}n and Abdalla, Safia and Willing, Carol},
doi = {10.3233/978-1-61499-649-1-87},
file = {:home/matt/Documents/Mendeley Desktop/Kluyver et al. - 2016 - Jupyter Notebooks—a publishing format for reproducible computational workflows.pdf:pdf},
isbn = {9781614996491},
issn = {0015-0193},
journal = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
keywords = {notebook,reproducibility,research code},
mendeley-groups = {2018-hackAuton},
pages = {87--90},
pmid = {23502158},
title = {{Jupyter Notebooks—a publishing format for reproducible computational workflows}},
year = {2016}
}

@incollection{Agresti2003,
author = {{Alan Agresti}},
publisher = {Wiley-Blackwell},
isbn = {9780471249689},
title = {Logistic Regression},
booktitle = {Categorical Data Analysis},
chapter = {5},
pages = {165-210},
doi = {10.1002/0471249688.ch5},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471249688.ch5},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471249688.ch5},
year = {2003},
keywords = {logistic regression, logit, odds ratio, Newton–Raphson method, binary data, likelihood‐ratio inference},
abstract = {Summary Chapter 5 focuses on the logistic regression model. It pays special attention to interpreting parameters. It discusses methods of inference for the parameters, and discusses extensions with multiple predictors, some of which may be categorical. The chapter closes with details about model fitting.}
}